\documentclass[10pt,twocolumn]{article}
\usepackage{geometry}
\geometry{verbose,headsep=3cm,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.0cm,rmargin=2.0cm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[font=small]{caption}
\usepackage{amsmath,amssymb,latexsym}
\usepackage{marvosym}
\usepackage{url}
\usepackage{lipsum}
\usepackage{bm}
\usepackage{float}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{epsf}
\usepackage{float}
\usepackage{mathpazo}
\usepackage{pifont}
\usepackage{wrapfig}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{framed}
\usepackage[utf8]{inputenc}
% Document font:
\usepackage{charter}
\graphicspath{{DWGs/}}

\begin{document}

\twocolumn[{
\begin{@twocolumnfalse}

  \begin{center}
%\textcolor{lgray}
    \vskip-5em

    \hfill
    \fontsize{10}{10}\selectfont {\textit{Bruxelles, September 2019 - October 2020}}

    \vskip2ex
    
	\vspace{5ex}
	
    \fontsize{24}{10}\selectfont {Notes on Dynamic Mode Decomposition}
    
    \fontsize{18}{10}\selectfont {(with some code)}



  \noindent%
    
\vskip1ex

{\rule{\textwidth}{0.5pt}}

  \end{center}
  
    \fontsize{7}{10}\selectfont {This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license.}

\vspace{6mm}

\end{@twocolumnfalse}
}]

\setlength{\parindent}{0cm}

\vspace{10mm}

\setlength{\parindent}{0cm}

\fontsize{14}{10}\selectfont {Kamila Zdyba≈Ç}

\vspace{2mm}

\fontsize{8}{10}\selectfont {\textit{\href{https://kamilazdybal.github.io/}{kamilazdybal.github.io}, kamila.zdybal@gmail.com}}

\vspace{2mm}

\section*{Preface}

\textit{Dynamic Mode Decomposition} (DMD) is a data-driven method for approximating a (generally) nonlinear dynamical system with a linear one. The technique can be used for performing low-rank approximation of a data set as well as for finding low-rank structures in multi-dimensional data sets.

These notes are additionally accompanied by a set of codes that aim to show DMD in action.

\,\,

We all stand on the shoulders of giants. This document combines the knowledge from various sources that shaped my journey through understanding DMD. The two main resources that pushed me to writing this document are: wonderful lectures by Professor Nathan Kutz from the University of Washington \cite{Prof_Nathan_Kutz_1}-\cite{Prof_Nathan_Kutz_2} and an excellent introductory textbook \textit{Invitation to dynamical systems} by Professor Edward Scheinerman from the John Hopkins University \cite{Prof_Edward_Scheinerman}. I would also like to express my gratitude to Professor Miguel A. Mendez with whom I took the first steps into this journey and discovered the power of data decomposition.

\,\,

Please feel free to contact me with any suggestions, corrections or comments.

\section*{Keywords}

\textit{Dynamic Mode Decomposition (DMD), linear algebra, matrix decomposition, matrix approximation, linear dynamical systems, complex plane}



\tableofcontents


\section{Setting the stage}

This small opening section is here to give you a bit of intuition for DMD which hopefully will be handy in understanding the mathematical formulation in the next chapters. It is assumed that you have some general idea on what it means to perform data decomposition.

The main purpose of DMD is to find \textit{dynamic modes} underlying the data set, in a purely data-driven way. The modes are \textit{dynamic} in a sense that they are spatial structures, each associated with a certain time evolution given by frequency of oscillations. In other words, in DMD we not only know how the system can be decomposed spatially (what spatial structures it is composed of), but also how each spatial structure behaves in time. These two concepts will go hand in hand in DMD, unlike in some other decomposition techniques you might know.

DMD is also a data reduction/approximation technique. The original data set can be reconstructed by summing up the dynamic modes, or approximated by summing up a \textit{significant portion} of the dynamic modes.

\begin{figure}[H]
\centering\includegraphics[width=5.5cm]{DMD-modal-decomposition.png}
\caption{Modal decomposition of a data matrix $\mathbf{D}$.}
\label{fig:modal-decomposition}
\end{figure}

In many data reduction techniques, in addition to reconstructing data from its modes, different data decomposition techniques can give a different meaning to the modes themselves. In general, for physical data sets such as ones coming from fluid dynamics, we would like the modes to have a physical interpretation (which is not \textit{a priori} guaranteed). That would allow to extract additional information about the physics of the original data set.

%Explain more. Give a more pictorial example of a meaningful mode. One more figure would be nice.

\subsection{Dynamic decomposition of a flipbook}

To see a pictorial example before we jump into the mathematical formalism, let's take a small detour and think about what it would mean to dynamically decompose a flipbook\footnote{a flipbook is a collection of pages that have consecutive snapshots of a scene drawn on them. As we flip the pages in our hands fast enough, we get an impression of watching a moving scene. If you would like to see for yourself, take a look at this YouTube video [\ref{bib:andymation}].}.

Let's say that each page is composed of a background which is the same in every slide (it is constant in time). There might also be an animation of moving objects happening in the foreground where some parts are animated as moving faster and some as moving slowly.

So what would be a dynamic mode in that example?

A mode can simply be such element of all the flipbook pages which has got the same behaviour in time. One mode that we may find would be the stationary background, another one would be an animated car moving from left to right on the pages. Notice that the dynamic mode is not an entire page (snapshot), but rather some portion of each page. As we sum all the spatial portions, each associated with its evolution in time, we get back the full flipbook animation.

We associate a certain number, say $\lambda$, to describe the evolution in time of every separate element (spatial structure) of the animation. This number will have a meaning of frequency of appearance.

And hence the background might be thought of as one spatial mode that has got a a constant time behaviour - it is present on every flipbook page. A ball bouncing on the foreground might be another mode, associated with its $\lambda$. 

The analogy presented in this section is based on \cite{Grosek}.

In order to attach a physical meaning to the spatial modes extracted, the DMD technique can distinguish a particular physical object as a separate dynamic mode (for instance: a jumping ball in the foreground, or a moving cloud in the background).

\section{Dynamical systems}

Dynamic Mode Decomposition connects strongly with the arguments made for \textit{dynamical systems} - systems that change in time. There is an interesting intuition carried from one-dimensional linear dynamical systems to multi-dimensional linear dynamical systems and we will briefly present it here.

This section gathers the knowledge from an excellent book \cite{Prof_Edward_Scheinerman} and I encourage you to look into Chapter 2 for a much more complete explanation.

%Introducing the dynamical systems will be a good tool to understand the underlying concepts of DMD, especially the role of eigendecomposition which will appear as we present the theory in section \ref{sec:theory}.

\subsection{One-dimensional linear dynamical system}

A discrete one-dimensional linear dynamical system is characterized by the following evolution:

\begin{equation}
x(i+1) = a x(i) + b
\end{equation}

where $x(i)$ and $x(i+1)$ are two consecutive measurements of the system and $a$ and $b$ are the system parameters.

If we know the initial condition of this system $x(0) = x_0$, we may also write that in the general case for $i$ equal to some $k$:

\begin{equation} \label{eq:1D-lin-dyn-sys-not-constant}
x(k) = a^k x_0 + \frac{a^k - 1}{a - 1} b
\end{equation}

for $a \neq 1$, or:

\begin{equation} \label{eq:1D-lin-dyn-sys-constant}
x(k) = x_0 + k b
\end{equation}

when $a = 1$.

Let's think now what could a linear dynamical system behaviour be? What are the possible ways in which the measured variable propagates? Surely it can stay constant in time, or if not, it can either grow or decay as we keep on measuring the system. The constant behaviour is captured by the solution described by eq.(\ref{eq:1D-lin-dyn-sys-constant}). The other two behaviours are encoded in eq.(\ref{eq:1D-lin-dyn-sys-not-constant}) - the character of the system's evolution in time is dependent on the parameter $a$.

For a deeper explanation of various cases I encourage you to take a look at Chapter 2 in \cite{Prof_Edward_Scheinerman}.

\subsection{Multi-dimensional linear dynamical system}

A discrete multi-dimensional linear dynamical system is, on the other hand, characterized by the following evolution:

\begin{equation}
\mathbf{x}(i+1) = \mathbf{A} \mathbf{x}(i) + \mathbf{b}
\end{equation}

The measurement $\mathbf{x}$ is no longer a single variable (a scalar) but an entire vector. It could come from measurements taken at different points on the mesh at a particular moment in time. Let's assume the vector size is $n$. We may call again $\mathbf{A}$ and $\mathbf{b}$ the parameters of the system but this time $\mathbf{A}$ is a matrix of size $(n \times n)$ and $\mathbf{b}$ is a vector of size $n$.

Can we find the analogous parameter to $a$ from the 1D dynamical systems that would define the time behaviour of a multi-dimensional one? Turns out this parameter are the eigenvalues of a matrix $\mathbf{A}$! If you are new to dynamical systems this probably seems like a very strange but also fortunate result. Again, for a thorough proof of that I encourage you to turn to Chapter 2 in \cite{Prof_Edward_Scheinerman}.

The most important thing to remember from this section is the following: \textbf{the behaviour of a multi-dimensional linear dynamical system is dependent on the eigenvalues of the system}. This is the piece of knowledge that will soon show prove important in the explanation of the steps of Dynamic Mode Decomposition.

\subsection{Solution to a dynamical system}

We have a system described by a differential equation:

\begin{equation} \label{eq:system_DE}
\frac{d \mathbf{x}(t)}{dt} = f(\mathbf{x}(t), t, \dots)
\end{equation}

where the function $f(\mathbf{x}(t), t, \dots)$ is a way of \textit{modeling} that system.

We also have collected \textit{measurements} of the system at different points in space at time $k$, in the form of a vector(s) $\mathbf{\mathbf{y}}_k$:

\begin{equation}
\mathbf{\mathbf{y}}_k = g(\mathbf{x}_k)
\end{equation}

\begin{wrapfigure}{R}{0.2\textwidth}
\centering\includegraphics[width=3.5cm]{data-matrix.png}
\caption{Data matrix with measurements of the system.}
\label{fig:data-matrix}
\end{wrapfigure}

where $\mathbf{x}_k$ is the quantity of interest that we are aiming at measuring. The fact that we might not be able to measure it directly is accounted for by some function $g()$ (although it might happen that $\mathbf{y}_k = \mathbf{x}_k$, meaning that we are able to measure $\mathbf{x}_k$ directly).

Notice, that for measurements at many moments in time, we may stack all the collected vectors $\mathbf{y}_i$ for different times $i$ to create a matrix whose columns represent time snapshots and whose rows represent position in space - see Figure \ref{fig:data-matrix}. This is a matrix that we are going to work with.

We are from now interested in systems where the governing equation from eq.(\ref{eq:system_DE}) is not known (in other words, the function $f$ is unknown) and we solely rely on measurements of the system which, in general, form a high-dimensional data set.

In the Dynamic Mode Decomposition we approximate that data set by a linear dynamical system of the form:

\begin{equation} \label{eq:system_linear}
\frac{d \mathbf{x}(t)}{dt} = \mathbf{A} \mathbf{x}(t)
\end{equation}

This is in fact a very handy approximation since we are able to write down exact solutions to linear systems.

Once we assume that the general solution is of the form:

\begin{equation} \label{eq:general_solution}
\mathbf{x}(t) = \mathbf{v} e^{\bm{\lambda} t}
\end{equation}

to obtain the parameters we effectively solve the eigenvalue problem:

\begin{equation} \label{eq:eigenvalue_solution}
\mathbf{A} \mathbf{v} = \bm{\lambda} \mathbf{v}
\end{equation}

The exact solution to the linear system from eq.(\ref{eq:system_linear}) is:

\begin{equation} \label{eq:soln_exact}
x = \sum_{j = 1}^{n} b_j \phi_j e^{\lambda_j t}
\end{equation}

For a reader interested in how this solution was derived, more can be found in appendix \ref{app:A}. 

\section{Dynamic Mode Decomposition theory}\label{sec:theory}

\subsection{Exact DMD}

For the moment, we assume that we can measure the system directly, that is we measure $\mathbf{y}_i = \mathbf{x}_i$. Moreover, we assume that our data is collected in equal\footnote{Which is indeed a special case for real life measurements. Check section \ref{sec:view} for more information.} time steps $\Delta t$. The measurements are combined inside a large matrix $\mathbf{X}$ where each of its columns represents one time snapshot:

\begin{equation} \label{eq:X}
\mathbf{X} = 
\begin{bmatrix}
    \mathbf{x}_1 & \mathbf{x}_2 & \mathbf{x}_3 & \dots & \mathbf{x}_{m}
\end{bmatrix}
\end{equation}

We split the large matrix $\mathbf{X}$ into two matrices $\mathbf{X_1}$ and $\mathbf{X_2}$ such that:

\begin{equation} \label{eq:X1}
\mathbf{X_1} = 
\begin{bmatrix}
    \mathbf{x}_1 & \mathbf{x}_2 & \mathbf{x}_3 & \dots & \mathbf{x}_{m-1}
\end{bmatrix}
\end{equation}

\begin{equation} \label{eq:X2}
\mathbf{X_2} = 
\begin{bmatrix}
    \mathbf{x}_2 & \mathbf{x}_3 & \mathbf{x}_4 & \dots & \mathbf{x}_{m}
\end{bmatrix}
\end{equation}

If we now assume that a linear operator will map the first element of $\mathbf{X_1}$ with the first element of $\mathbf{X_2}$, second with the second, third with the third, and so on, matrix $\mathbf{X_2}$ can be thought of as a matrix representing the \textit{future state} of the matrix $\mathbf{X_1}$. That linear operator is assumed to be a matrix $\mathbf{A}$. 

\begin{figure}[H]
\centering\includegraphics[width=5.5cm]{data-split.png}
\caption{Splitting the data matrix into \textit{past} and \textit{future} matrices $\mathbf{X_1}$ and $\mathbf{X_2}$, linked by the linear operator $\mathbf{A}$.}
\label{fig:linear_system}
\end{figure}

Note here, that for nonlinear systems, a matrix that transforms $\mathbf{x}_1$ to $\mathbf{x}_2$ is different from a matrix that transforms $\mathbf{x}_2$ to $\mathbf{x}_3$ and so on. DMD assumes, however, that there is one matrix $\mathbf{A}$ that does all these transformations at once, with the least amount of error. It finds the \textit{best-fit} linear dynamical system for the non-linear data set. In mathematical terms, we are looking for such $\mathbf{A}$ that:

\begin{equation} \label{eq:linear_dynamics}
\mathbf{X_2} = \mathbf{A} \mathbf{X_1}
\end{equation}

At this point, we should pause and appreciate what the above equation is representing. It describes a linear system of equations which (in the most practical case) will be underdetermined. The unknown in this equation is the matrix $\mathbf{A}$, hence the number of unknowns is equal to the number of elements of this matrix: $n_s \cdot n_s$. The number of linear equations however is $n_s \cdot n_t$ and in many dynamical systems of interest $n_s >> n_t$. Such system might have infinitely many solutions and therefore some additional constraint on $\mathbf{A}$ is needed in order to select one solution out of infinitely many. This boils down to the problem of regression - what regression condition we chose is up to us. In particular, we might restrict ourselves to such $\mathbf{A}$ that will minimize the L2 norm. It is worth noting that there might be different \textit{best} $\mathbf{A}$ that gets us from snapshot $5$ to $6$ then the one that gets us from snapshot $100$ to $101$. What our regression will aim at optimizing is to find one matrix $\mathbf{A}$ that \textit{sort of} does all the past-future links in the \textit{best} way it can.

If we chose to proceed with the regression that minimizes the L2 norm (as is typically done in DMD algorithm) the solution to the system of eq.(\ref{eq:linear_dynamics}) is obtained by multiplying both sides by the \textit{pseudo-inverse} of matrix $\mathbf{X_1}$ which we denote by $\mathbf{X_1}^{+}$:

\begin{equation} \label{eq:linear_dynamics_A}
\mathbf{A} = \mathbf{X_2} \mathbf{X_1}^{+}
\end{equation}

The pseudo-inverse described here, also known as the Moore-Penrose inverse\footnote{Check appendix \ref{app:B} for more information.}, is computed using the least squares method. There is therefore certain information lost when going from eq.(\ref{eq:linear_dynamics}) to eq.(\ref{eq:linear_dynamics_A}).

Once we have solved for matrix $\mathbf{A}$, we can go back to eq.(\ref{eq:eigenvalue_solution}) and solve for eigenvalues and eigenvectors.

Up to this point, this is what the \textbf{exact DMD} computes. There is however a problem that the eq.(\ref{eq:linear_dynamics_A}) may pose when numerics are involved and this will be addressed in the next section. 

\subsection{Going low-rank}

Matrices $\mathbf{X_1}^{+}$ and $\mathbf{X_2}$ typically represent huge spacial dimensionality\footnote{This is often the case for data sets where we have very few snapshots in time but a large number of spacial points where the measurements were taken. Graphically, we might think of those matrices as being "tall" and this is illustrated in Figure \ref{fig:data-matrix}.} which in turn means that the matrix $\mathbf{A}$ can become a square matrix of a massive size. 

\begin{wrapfigure}{R}{0.2\textwidth}
\centering\includegraphics[width=3.5cm]{getting-A.png}
\caption{Building the linear operator $\mathbf{A}$ in exact DMD.}
\label{fig:building-A}
\end{wrapfigure}

We are hence reluctant to perform the multiplication of matrices as is stated in eq.(\ref{eq:linear_dynamics_A}). 

The hope comes from the \textit{Singular Value Decomposition} (SVD). We belive that there are low-rank structures hidden in the data set and we are able to reduce the dimensionality of matrix $\mathbf{A}$ without significant loss of information\footnote{Professor Kutz said a very interesting sentence here, that the multiplication presented in Figure \ref{fig:building-A} completely ignores the fact that there might be low-rank structures in our data set.}.

We perform the SVD on matrix $\mathbf{X_1}$ and decompose it to the component matrices: $\mathbf{U}$, $\mathbf{\Sigma}$ and $\mathbf{V}$.

\begin{equation} \label{eq:solution}
\mathbf{X_1} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T 
\end{equation}

Based on the rank structure of the matrix $\mathbf{X_1}$ (one way to get information about the rank structure is to plot the elements from the diagonal of the matrix $\mathbf{\Sigma}$) we perform a rank-$r$ truncation on the SVD decomposition and approximate the matrix $\mathbf{X_1}$ by its low-rank (rank-$r$) representation:

\begin{equation} \label{eq:solution-approx}
\mathbf{X_1} \approx \mathbf{X_{1r}} = \mathbf{U_r} \mathbf{\Sigma_r} \mathbf{V_r}^T 
\end{equation}

The pseudo-inverse of the truncated matrix is:

\begin{equation} \label{eq:pseudo-inverse}
\mathbf{X_{1r}}^{+} = \mathbf{V_r}  \mathbf{\Sigma_r}^{-1} \mathbf{U_r}^T
\end{equation}

The usefulness of this decomposition might not yet be evident, since the matrix $\mathbf{X_{1r}}$ is of the same size as matrix $\mathbf{X_{1}}$, they only differ by rank. The idea is to nevertheless use the SVD decomposition but also, to generate a matrix similar to the matrix $\mathbf{A}$ (since similar matrices share eigenvalues and eigenvectors, among some other properties) but one that will have a smaller size (in fact, it will be size $(r \times r)$). This similar matrix will be denoted $\underline{\mathbf{A}}$. Since it has a lower size than the original matrix $\mathbf{A}$, we will only retrieve $r$ eigenvectors and eigenvalues.

\begin{wrapfigure}{R}{0.25\textwidth}
\centering\includegraphics[width=4cm]{similar-matrices.png}
\caption{Similarity transform of matrix $\mathbf{A}$ to reduce its size.}
\label{fig:similar-matrices}
\end{wrapfigure}

What will now follow are clever mathematical steps performed to avoid computation of the large matrix $\mathbf{A}$.

We come back to the eq.(\ref{eq:linear_dynamics_A}) and

We perform a \textit{similarity transform} of the matrix $\mathbf{A}$:

\begin{equation} \label{eq:similarity-transform}
\underline{\mathbf{A}} = \mathbf{U_r}^T \mathbf{A} \mathbf{U_r}
\end{equation}

Matrix $\mathbf{A}$ can be written as:

\begin{equation} \label{eq:A}
\mathbf{A} = \mathbf{X_2} \mathbf{V_r} \mathbf{\Sigma_r}^{-1} \mathbf{U_r}^T
\end{equation}

The similar matrix $\underline{\mathbf{A}}$ can be written as:

\begin{equation} \label{eq:A_underline}
\underline{\mathbf{A}} = \mathbf{U_r}^T \mathbf{X_2} \mathbf{V_r} \mathbf{\Sigma_r}^{-1} 
\end{equation}

taking into account that $\mathbf{U_r}^T \mathbf{U_r} = \mathbf{I}$.

We have thus chosen a low-dimensional subspace by performing rank-$r$ truncation in which we now find the solution to the linear dynamical system presented initially. The solution is built in this low-dimensional subspace.

\subsection{Eigendecomposition}

Now that we have computed the similar matrix $\underline{\mathbf{A}}$, we move on to perform the eigendecomposition:

\begin{equation} \label{eq:A_underline}
[\mathbf{W}, \mathbf{\Lambda}] = \text{eig}(\underline{\mathbf{A}})
\end{equation}

The matrix $\mathbf{W}$ is a matrix whose columns are the eigenvectors of $\underline{\mathbf{A}}$. The matrix $\mathbf{\Lambda}$ is a diagonal matrix of eigenvalues $\lambda_i$ which, in the most general case, are complex numbers. We have therefore obtained "coupled" quantities - each eigenvector has got its corresponding eigenvalue - and that link is exploited by the Dynamic Mode Decomposition to give a physical meaning to eigenvectors and eigenvalues.

The eigenvectors will form the spacial modes of DMD. 

We can write the complex eigenvalue as:

\begin{equation} \label{eq:A_underline}
\lambda_i = e^{(\sigma_i + i \omega_i) \Delta t} 
\end{equation}

The real part of the eigenvalue corresponds to the growth rate of that particular mode and the imaginary part represents the oscillatory frequency of a mode.

\subsection{Going back to the original dimensions}

In the previous section we have retrieved $r$ eigenvalues and eigenvectors but our original dimension for the propagator matrix was $n_t$. We have reduced the dimensionality of the problem but our final solution will only make sense to us if it is expressed in $n_t$ dimensions.
Once the model has been built in the low-dimensional subspace, we want to move to the original dimensions. 

The DMD modes are obtained from:

\begin{equation} \label{eq:A_underline}
\mathbf{\Phi} = \mathbf{X_2} \mathbf{V} \mathbf{\Sigma}^{-1} \mathbf{W}
\end{equation}

The above equation can be interpreted as transforming the eigenvectors matrix $\mathbf{W}$ to a new \textit{basis vectors} matrix $\mathbf{\Phi}$ where the linear transformation is given by the composition: $\mathbf{X_2} \mathbf{V} \mathbf{\Sigma}^{-1}$.

What is worth mentioning here, is that DMD modes are not guaranteed to be orthogonal after such transformation. This creates a great capacity of DMD to be applicable to systems where data structure does not exhibit orthogonality.

\begin{figure}[H]
\centering\includegraphics[width=8cm]{dmd-modes-transformation.png}
\caption{Every of the $r$ vectors in the eigenvectors matrix $\mathbf{W}$ gets transformed into one of the $r$ vectors in the DMD modes matrix $\mathbf{\Phi}$. $\mathbf{M}$ is the matrix associated with this transformation. We have moved from the $r$-dimensional space onto the original space.}
\label{fig:linear_system}
\end{figure}

\subsection{DMD solution}

The solution to the original dynamical system is finally computed:

\begin{equation} \label{eq:x-solution-matrix}
\mathbf{x}(t) = \mathbf{\Phi} e^{\mathbf{\Omega} t} \mathbf{b}
\end{equation}

the above equation is equivalent to:

\begin{equation} \label{eq:x-solution-summation}
\mathbf{x}(t)  = \sum_{k=1}^{r} \phi_k e^{\omega_k t} b_k
\end{equation}

The vector $\mathbf{b}$ is a vector of initial amplitudes for each mode. It is computed by projecting the initial state of the data matrix (given by $\mathbf{x}_1$) onto the DMD modes. This is done in order for our initial condition to be formulated in the obtained DMD basis.



%\section{A broader view} \label{sec:view}
%
%What can go different with our data sets?
%
%\subsection{Optimized DMD}
%
%- varying time steps
%
%We mentioned earlier, that 
%
%\subsection{Robust DMD}
%
%
%
%Sparse Identification
%
%
%
%
%\subsection{Multi-diagnostics DMD}
%
%\section{Python example}

\newpage

\section*{Appendix}

\appendix

\section{Solution to linear dynamical systems} \label{app:A}

This section is based on Chapter 6 of \cite{Prof_Gilbert_Strang} where the solution to systems of differential equations is presented.

We first recall the general solution to the differential equation:

\begin{equation} \label{eq:DE_exp}
\frac{d f(x)}{dt} = f(x)
\end{equation}

to be the exponential function: $f(x) = a \cdot e^{x}$. This is a single differential equation.


In an analogous way, the general solution to the system of differential equations of the form:

\begin{equation} \label{eq:system_linear_A}
\frac{d \mathbf{x}}{dt} = \mathbf{A} \mathbf{x}
\end{equation}

where the vector $\mathbf{x}$ is unknown is:

\begin{equation} \label{eq:general_solution_A}
\mathbf{x} = \mathbf{v} e^{\bm{\lambda} t}
\end{equation}

Notice that the above system of equations is discrete.

Computing the time derivative of the eq.(\ref{eq:general_solution_A}) we get:

\begin{equation} \label{eq:sub1}
\frac{d \mathbf{x}}{dt} = \mathbf{v} \bm{\lambda} e^{\bm{\lambda} t}
\end{equation}

And substituting the eq. \ref{eq:general_solution_A} to eq. \ref{eq:system_linear_A} we get:

\begin{equation} \label{eq:sub2}
\frac{d \mathbf{x}}{dt} = \mathbf{A} \mathbf{v} e^{\bm{\lambda} t}
\end{equation}

The nontrivial solution for the equality of these two above equations is obtained when:

\begin{equation} \label{eq:eigval}
\mathbf{A} \mathbf{v} = \bm{\lambda} \mathbf{v} 
\end{equation}

which is the statement of eigenvalue problem.



\begin{figure}[H]
\centering\includegraphics[width=5cm]{lin-dyn.png}
\caption{Linear dynamical system.}
\label{fig:linear_system}
\end{figure}

%\section{Moore-Penrose inverse} \label{app:B}

\newpage

\section{Singular Value Decomposition} \label{app:C}

Here we present a pictorial representation of Singular Value Decomposition of a data matrix $\mathbf{X}$ for your reference. The red matrices are truncated to keep only $r$ first components and their multiplication results in a rank-$r$ approximation to the original matrix $\mathbf{X}$.

\begin{figure}[H]
\centering\includegraphics[width=8cm]{svd.png}
\caption{Sizes of component matrices in the Singular Value Decomposition and after rank truncation.}
\label{fig:linear_system}
\end{figure}


\thebibliography{}



\bibitem{Prof_Nathan_Kutz_1} N. Kutz, \textit{Dynamic Mode Decomposition Theory}, an online lecture: \verb|https://youtu.be/bYfGVQ1Sg98| \label{bib:kutz_1}

\bibitem{Prof_Nathan_Kutz_2} N. Kutz, \textit{Dynamic Mode Decomposition Code}, an online lecture: \verb|https://youtu.be/KAau5TBU0Sc| \label{bib:kutz_2}

\bibitem{Andymation} \verb|https://www.youtube.com/watch?v=ZCCETV-8950| \label{bib:andymation}

\bibitem{Prof_Edward_Scheinerman} E. R. Scheinerman, \textit{Invitation to Dynamical Systems}

\bibitem{Grosek} J. Grosek, N. Kutz, \textit{Dynamic Mode Decomposition for Real-Time Background/Foreground Separation in Video}, 2014

\bibitem{Prof_Gilbert_Strang} G. Strang, \textit{Introduction to Linear Algebra}, 5th edition

\bibitem{Zdybal} K. Zdybal, M. A. Mendez, \textit{POD and DMD decomposition of numerical and experimental data}, von Karman Institute for Fluid Dynamics, 2016


\end{document}
